{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "##  Q-learning with Linear Value Function Approximation   ##\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Transition probabilities\n",
    "# p_zero = 0.5  # 50% probability of zero inflation\n",
    "# dist_name = 'poisson'  # Base distribution\n",
    "# lambda_poisson = 4  # Poisson mean\n",
    "\n",
    "# def zero_inflated_prob_vector(p_zero, dist_name, dist_params, s):\n",
    "#     \"\"\"\n",
    "#     Computes a probability vector for a zero-inflated random variable.\n",
    "\n",
    "#     Parameters:\n",
    "#     - p_zero (float): Probability of zero inflation.\n",
    "#     - dist_name (str): Name of the base distribution ('poisson', 'nbinom', 'binom').\n",
    "#     - dist_params (tuple): Parameters of the base distribution.\n",
    "#     - s (int): Threshold for \"s or greater\" category.\n",
    "\n",
    "#     Returns:\n",
    "#     - np.array: Probability vector of length (s+1) where:\n",
    "#         - First element: P(X=0)\n",
    "#         - Second element: P(X=1)\n",
    "#         - ...\n",
    "#         - Second-to-last element: P(X=s-1)\n",
    "#         - Last element: P(X >= s)=1-(P(X=0)+...+P(X=s-1))\n",
    "#     \"\"\"\n",
    "#     # Get the chosen probability mass function (PMF)\n",
    "#     base_dist = getattr(stats, dist_name)\n",
    "\n",
    "#     if s==0:\n",
    "#         prob_vector = [p_zero]\n",
    "#     else:\n",
    "#         # Compute probabilities for values 0 to (s-1)\n",
    "#         pmf_values = (1 - p_zero) * base_dist.pmf(np.arange(s), *dist_params)\n",
    "        \n",
    "#         # Adjust probability of zero (includes zero-inflation)\n",
    "#         pmf_values[0] += p_zero\n",
    "        \n",
    "#         # Compute probability for X â‰¥ s\n",
    "#         p_s_or_more = 1 - np.sum(pmf_values)\n",
    "        \n",
    "#         # Append P(X >= s) as the last element\n",
    "#         prob_vector = np.append(pmf_values, p_s_or_more)\n",
    "    \n",
    "#     return prob_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility\n",
    "np.random.seed(0)\n",
    "\n",
    "def plot_episode_returns(results, title=\"Episode Return Convergence\"):\n",
    "    \"\"\"\n",
    "    Utility function to plot episode returns (can be multiple).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    for label, data in results.items():\n",
    "        plt.plot(data[\"episode_returns\"], label=label)\n",
    "\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Episode Return\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def print_policies_and_q_values(Q, label=None):\n",
    "    \"\"\"\n",
    "    Utility function to print policies and corresponding Q values (can be multiple)\n",
    "    \"\"\"\n",
    "    if label:\n",
    "        print(f\"\\n===== Optimal Policy for {label} =====\")\n",
    "    print(\"Optimal Policy (0 = Do nothing, 1 = Maintenance):\")\n",
    "    for comp_type in range(len(xi)):\n",
    "        policy = np.argmax(Q[comp_type], axis=0)\n",
    "        policy[xi[comp_type]] = 1  # Force maintenance at threshold\n",
    "        print(f\"Component Type {comp_type + 1} (Failure Threshold = {xi[comp_type]}):\")\n",
    "        print(policy)\n",
    "\n",
    "    print(\"\\nQ-values for each Component Type:\")\n",
    "    for comp_type in range(len(xi)):\n",
    "        print(f\"\\nComponent Type {comp_type + 1} (Failure Threshold = {xi[comp_type]}):\")\n",
    "        print(Q[comp_type])\n",
    "\n",
    "# Define MDP parameters\n",
    "actions = (0, 1)  # (0=do nothing, 1=maintenance)\n",
    "xi = (15, 30, 50) # break threshold per type of component\n",
    "S = [list(range(x + 1)) for x in xi]\n",
    "C = tuple([[0, 1]] * x + [[math.inf, 5]] for x in xi)\n",
    "gamma = 0.9  # Discount factor\n",
    "\n",
    "def choose_action(Q: tuple, comp_type: int, s: int, xi: tuple, epsilon: float) -> int:\n",
    "    \"\"\"\n",
    "    Choose an action using an epsilon-greedy policy with a forced action at the threshold.\n",
    "\n",
    "    Parameters:\n",
    "     - Q         : Tuple of numpy arrays representing the Q-table for each component type.\n",
    "     - comp_type : Integer index indicating the component type.\n",
    "     - s         : Current state (an integer) for the selected component type.\n",
    "     - xi        : Tuple containing the failure threshold for each component type.\n",
    "     - epsilon   : Exploration probability.\n",
    "\n",
    "    Returns:\n",
    "     - a         : The chosen action (0 or 1). If s equals the threshold (xi[comp_type]), then returns 1.\n",
    "    \"\"\"\n",
    "    # If state equals the failure threshold, force the maintenance action.\n",
    "    if s == xi[comp_type]:\n",
    "        return 1\n",
    "\n",
    "    # generate random float in the half-open interval [0.0, 1.0)\n",
    "    if np.random.random() < epsilon:\n",
    "        # if less than epsilon, choose a random action (exploration)\n",
    "        return np.random.randint(2)\n",
    "    else:\n",
    "        # if greater than epsilon, choose the best action (exploitation)\n",
    "        return np.argmax(Q[comp_type][:, s])\n",
    "\n",
    "\n",
    "def run_simulation(nEpisodes, lengthEpisode, initial_epsilon, min_epsilon=0.01, decay_rate=5000,\n",
    "                   patience=None):\n",
    "    \n",
    "    # ----- initialization -----\n",
    "\n",
    "    #* initialize Q(s,a) arbitrarily for all s in S and a in A_s\n",
    "    Q = tuple(np.zeros((2, x + 1)) for x in xi)\n",
    "    #* Keep track of the number of visits N(s,a)\n",
    "    N = tuple(np.zeros((2, x + 1)) for x in xi)\n",
    "\n",
    "    prev_policy = None\n",
    "    stable_count = 0\n",
    "\n",
    "    episode_returns = []\n",
    "    \n",
    "    for episode_nr in tqdm(range(nEpisodes), desc=\"Episodes\"):\n",
    "        \n",
    "        # decay epsilon\n",
    "        epsilon = max(min_epsilon, initial_epsilon * np.exp(-episode_nr / decay_rate))\n",
    "\n",
    "        # ----- Generate Episode -----       \n",
    "        \n",
    "        # initialize S\n",
    "        comp_type = np.random.randint(0, 3)\n",
    "        s = 0 #S[comp_type][0]\n",
    "\n",
    "        episode_transitions = []  # List to store transitions of the current episode\n",
    "\n",
    "        # for each step:\n",
    "        for step in range(lengthEpisode):\n",
    "            # choose action A using e-greedy policy w.r.t. current Q\n",
    "            a = choose_action(Q, comp_type, s, xi, epsilon)\n",
    "\n",
    "            # take action A, observe S'\n",
    "            if a == 0:\n",
    "                prob_vector = zero_inflated_prob_vector(p_zero, dist_name, (lambda_poisson,), xi[comp_type]-s)\n",
    "                increments = np.arange(len(prob_vector))\n",
    "                increment = np.random.choice(increments, p=prob_vector)\n",
    "                s_prime = s + increment\n",
    "                if s_prime > xi[comp_type]:\n",
    "                    s_prime = xi[comp_type]\n",
    "                comp_type_prime = comp_type\n",
    "            else:\n",
    "                s_prime = 0\n",
    "                comp_type_prime = np.random.randint(0, 3)\n",
    "\n",
    "            # observe R\n",
    "            r = -C[comp_type][s][a]\n",
    "\n",
    "            # append (S, A, R) to a list of episode transitions\n",
    "            episode_transitions.append((comp_type, s, a, r))\n",
    "\n",
    "            # transition to S'\n",
    "            s = s_prime\n",
    "            comp_type = comp_type_prime\n",
    "\n",
    "        # ----- Compute returns and update Q -----\n",
    "        # precompute all returns G_t for the episode in a backward pass\n",
    "        T = len(episode_transitions) # = lengthEpisode in this case bc no terminal state\n",
    "        G_t = np.zeros(T)\n",
    "        G = 0.0\n",
    "        for t in reversed(range(T)):\n",
    "            # unpack the transition tuple\n",
    "            _, _, _, r = episode_transitions[t]\n",
    "\n",
    "            # recursive computation of G_t\n",
    "            G = r + gamma*G\n",
    "            G_t[t] = G\n",
    "\n",
    "        episode_returns.append(G_t[0])\n",
    "\n",
    "        # every-visit Monte Carlo: for each state S_t and action A_t in the episode, update Q:\n",
    "        for t,step in enumerate(episode_transitions):\n",
    "            # unpack the transition tuple\n",
    "            comp_type, s, a, _ = step\n",
    "\n",
    "            # update the visit count: N(s_t, a_t) <- N(s_t, a_t) + 1\n",
    "            N[comp_type][a,s] += 1\n",
    "\n",
    "            # update the Q-value: Q(s_t, a_t) <- Q(s_t, a_t) + (1/N(s_t, a_t)) * (G_t - Q(s_t, a_t))\n",
    "            Q[comp_type][a,s] += (1/N[comp_type][a,s]) * (G_t[t] - Q[comp_type][a,s])\n",
    "\n",
    "\n",
    "        # ----- Early-stopping based on policy stability -----\n",
    "        \n",
    "        if patience is not None:\n",
    "            current_policy = tuple(np.argmax(Q[comp], axis=0) for comp in range(len(Q)))\n",
    "            \n",
    "            # Force maintenance at the threshold\n",
    "            for k, policy in enumerate(current_policy):\n",
    "                policy[xi[k]] = 1  \n",
    "\n",
    "            # Check if the policy is stable\n",
    "            if prev_policy is not None and all(np.array_equal(cp, pp) for cp, pp in zip(current_policy, prev_policy)):\n",
    "                stable_count += 1\n",
    "                if stable_count >= patience:\n",
    "                    print(f\"Stopped early at episode {episode_nr+1} â€” policy stable for {patience} episodes.\")\n",
    "                    return Q, episode_returns\n",
    "            else:\n",
    "                stable_count = 0\n",
    "                prev_policy = current_policy\n",
    "        \n",
    "    return Q, episode_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wbase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
